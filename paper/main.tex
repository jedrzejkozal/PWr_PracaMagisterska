\documentclass[a4paper, 10 pt, conference]{ieeeconf}
\overrideIEEEmargins
\usepackage{polski}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage[english]{babel}
\usepackage{hyperref}

\newcommand{\bb}{\textbf}

% Listingi
\usepackage{listings}
\usepackage{xcolor}
\lstdefinestyle{mystyle}{
	backgroundcolor=\color{gray!5!white},
	commentstyle=\color{green!50!black},
	keywordstyle=\color{magenta},
	numberstyle=\tiny\color{black!50!white},
	stringstyle=\color{purple},
	basicstyle=\footnotesize,
	breakatwhitespace=false,
	breaklines=true,
	captionpos=b,
	keepspaces=true,
	numbers=left,
	numbersep=5pt,
	showspaces=false,
	showstringspaces=false,
	showtabs=false,
	tabsize=2
}
\lstset{style=mystyle}

% The following packages can be found on http:\\www.ctan.org
\usepackage{graphics} % for pdf, bitmapped graphics files
\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{tikz}
\usetikzlibrary{positioning} 

\title{\LARGE \bf
Analysis of the effectiveness of recursive
networks in the classification task
}

\author{\parbox{2 in}{\centering Jędrzej Kozal \\
        Wrocław University of Science and Technology\\
        {\tt\small 218557@student.pwr.edu.pl}}
}


\begin{document}

\maketitle
\thispagestyle{empty}
\pagestyle{empty}

\selectlanguage{english}
\begin{abstract}

Recurrent Neural Networks are class of models designed to process sequences. Most of typical fields of applications include natural language processing, recognition of handwriting and generation of text, music or images. In this work emphasis was put on a ReNet architecture designed to solve an image classification task. A modification based on a Hilbert curve was introduced to the ReNet and obtained accuracy was very close to results acquired for the original ReNet network. The modification also provided significant training time reduction for some datasets. Comparison of ReNet networks to convolutional networks proved that the latter are superior.

\end{abstract}


\section{INTRODUCTION}

\cite{Goodfellow-et-al-2016}

 

\section{RELATED WORK}

In \cite{DBLP:journals/corr/VisinKCMCB15} ReNet architecture was introduced. It is alternative to convolutional networks, that enables to learn representation of an image, that can be used for classification purpose. Convolutional network computes activation based on the filters applied locally to part of an image. ReNet by using 4 recurrent neural networks can incorporate information scattered across whole image.

For convenience from now on we will refer to input as an image, but it can be also output of previous layer. We can describe inputs as tensor $X = \{x_{i,j}\}, X \in \mathbb{R}^{w \textrm{x} h \textrm{x} c}$, where $w, h$ are size of an image and $c$ is a number of channels. Image is divided into block of pixels called patches. Every patch is of size: $w_p$, $h_p$, therefore there are $(I \times J),I=\frac{w}{w_p}, J=\frac{h}{h_p}$ patches in the whole image. Set of all patches in image $X$ is defined as $P = \{p_{i,j}\}, P \in \mathbb{R}^{w_p \textrm{x} h_p \textrm{x} c}$. In first part of ReNet algorithm we take each columns of patches and feed it to 2 recurrent neural networks: 

\begin{gather}
	v_{i,j}^{F} = f_{VFWD} (v_{i,j-1}^F, p_{i,j}), \\
    v_{i,j}^{R} = f_{VREV} (v_{i,j+1}^R, p_{i,j}),
\end{gather}

where $f_{VFWD} (v_{i,j-1}^F, p_{i,j})$ can be activation of plain RNN, LSTM cell or GRU cell. Concatenated activation can be described as $V = \{v_{i,j}\}_{i=1,...,I}^{j=1,...,J}$. Tensor $V$ is input of another two recurrent layer networks, swapping rows from left to right, and right to left. Operation is analogues to vertical swap, with $f_{HFWD}, f_{HREV}$ computing activations $H = \{h_{i,j}\}$. Transformation $\Phi$ of each layer can be described with introduced designation as:

\begin{equation}
	\Phi: X \rightarrow V \rightarrow H
\end{equation}

Output of layer can be feed to another ReNet layer, or can flatted and feed to fully connected layer.

\section{METHODS}

\subsection{Hilbert curve}

\begin{figure}
	\centering
	\begin{tikzpicture}
	\node (h1) {$ $};
	\node (h2) [right=1.5cm of h1] {$ $};
	\node (h3) [above=1.5cm of h1] {$ $};
	\node (h4) [right=1.5cm of h3] {$ $};
		\draw (h1) -- (h3);
		\draw[-] (h3) edge node {$ $} (h4);
		\draw[-] (h4) edge node {$ $} (h2);
	\end{tikzpicture}
\caption{First 3 elements of sequence creating Hilbert Curve.}
	\label{fig:hilbert}
\end{figure}

Hilbert Curve $\mathcal{H}$ is space filling curve with fractal structure. It is defined by sequence of curves defined recursively, what be described as $k(n+1) = f(k(n))$. In this case transformation $f$ compounds of duplicating and rotating of $n$-th degree curve. First 3 elements of sequence creating Hilbert Cureve are presented in \ref{fig:hilbert}.

We can obtain Hilbert Curve in limit:

\begin{equation}
	\mathcal{H} = \lim_{n \rightarrow + \infty } k(n)
\end{equation}

By doing so, we can fill every point of unit square, hence name of this mathematical object - space filling curve. Hilbert Curve have other interesting quantities. $N$-th curve from sequence defining Hilbert Curve provides method of traversing image with side of length $2^{N}$. This can be utilized to define mapping from 2D to 1D and inverse. This mapping have one useful property. The same regions of an image are mapped to similar segments of line, regardless what the degree of this curve is. This is showed in \ref{fig:hilbert} above each curve.


\section{EXPERIMENT SETUP}

\section{RESULTS}

\section{CONCLUSIONS}


Code used for perforing studies is avaliable online in \cite{repo}.


\bibliographystyle{unsrt}
\bibliography{refs}

\end{document}
